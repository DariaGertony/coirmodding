# Simple Search Methods Evaluation

A straightforward script to test all search methods in the refactored CoIR architecture.

## ğŸš€ Quick Start

```bash
# Run the simple evaluation
python3 simple_search_evaluation.py
```

## ğŸ“‹ What it does

The script follows the same pattern as the README examples and tests:

1. **Dense Search** - Original CoIR semantic search
2. **Jaccard Similarity** - Lexical search using Jaccard
3. **BM25 Algorithm** - Lexical search using BM25
4. **Simple Hybrid** - Combination of dense + lexical
5. **Advanced Hybrid** - Advanced fusion with RRF

## ğŸ“Š Output

```
ğŸš€ CoIR Search Methods Evaluation
==================================================
ğŸ“¦ Loading model: intfloat/e5-base-v2
ğŸ“‹ Loading tasks...

ğŸ” Testing 5 search methods:
--------------------------------------------------
[1/5] Dense Search (Original)
  âœ… Success (15.23s)
[2/5] Jaccard Similarity
  âœ… Success (2.45s)
[3/5] BM25 Algorithm
  âœ… Success (3.12s)
[4/5] Simple Hybrid
  âœ… Success (18.67s)
[5/5] Advanced Hybrid (RRF)
  âœ… Success (21.34s)

ğŸ“Š Results Summary:
--------------------------------------------------
âœ… Dense Search (Original) (15.23s)
âœ… Jaccard Similarity (2.45s)
âœ… BM25 Algorithm (3.12s)
âœ… Simple Hybrid (18.67s)
âœ… Advanced Hybrid (RRF) (21.34s)

ğŸ¯ Final Score: 5/5 methods successful
ğŸ’¾ Results saved to: results/simple_evaluation_results.json
ğŸ‰ All search methods working correctly!
```

## ğŸ“ Generated Files

- `results/simple_evaluation_results.json` - Complete results in JSON format
- `results/dense/` - Dense search detailed results
- `results/jaccard/` - Jaccard search detailed results  
- `results/bm25/` - BM25 search detailed results
- `results/simple_hybrid/` - Simple hybrid detailed results
- `results/advanced_hybrid/` - Advanced hybrid detailed results

## ğŸ”§ Code Pattern

The script follows the exact same pattern as the README:

```python
# Load the model (same as README)
model = YourCustomDEModel(model_name="intfloat/e5-base-v2")

# Get tasks (same as README)
tasks = get_tasks(tasks=["codetrans-dl"])

# Initialize evaluation (same as README)
evaluation = COIR(tasks=tasks, batch_size=32, search_config=search_config)

# Run evaluation (same as README)
results = evaluation.run(model, output_folder=f"results/{method}")
```

## âœ¨ Features

- **Simple**: Just run one command
- **Clear Output**: Easy to read progress and results
- **Error Handling**: Shows which methods work and which need fixes
- **JSON Results**: Machine-readable output for further analysis
- **README Compatible**: Uses the exact same API as documented

This script validates that the refactored CoIR architecture works correctly and all search methods are properly integrated.