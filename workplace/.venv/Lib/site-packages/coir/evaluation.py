import os
import json
import logging
import torch
from coir.beir.retrieval.evaluation import EvaluateRetrieval
from coir.beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES


logger = logging.getLogger(__name__)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class COIR:
    def __init__(self, tasks, batch_size, type="semantic"):
        self.tasks = tasks
        self.batch_size = batch_size
        self.type = type
        #######################################
        

    def run(self, model, output_folder: str, useLLm: bool, llmname: str, prompt: str,to_rerank: bool):
        results = {}
        for task_name, task_data in self.tasks.items():
            output_folder = os.path.join(output_folder, f"{task_name}/{model.model_name}")
            output_file = os.path.join(output_folder, f"{self.type}.json")


            corpus, queries, qrels = task_data

            # Initialize custom model
            custom_model = DRES(model, batch_size=self.batch_size, type=self.type)
            retriever = EvaluateRetrieval(custom_model, score_function="cos_sim")

    
            # Retrieve results
            task_results = retriever.retrieve(corpus, queries, useLLm, llmname, prompt)


            # Evaluate results
            ndcg, map, recall, precision = retriever.evaluate(qrels, task_results, retriever.k_values)
            metrics = {
                "NDCG": ndcg,
                "MAP": map,
                "Recall": recall,
                "Precision": precision
            }

            
           

            
            os.makedirs(output_folder, exist_ok=True)

            with open(output_file, 'w') as json_file:
                json.dump({"metrics": metrics}, json_file, indent=4)

            logger.info(f"Results for {task_name} saved to {output_folder}")
            results[task_name] = metrics

        return results