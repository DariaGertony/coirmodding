from .. import BaseSearch
from .util import cos_sim, dot_score
import logging
import torch
from typing import Dict
import heapq
##################
import ollama
import threading
##################



######################

class AsyncOllama:
    def __init__(self, llmname):
        self.result = None
        self.threads = []
        self.llm = llmname
    
    def ask_async(self,i, query, callback=None):
        def worker():
            response = ollama.chat(
                 model=self.llm,
                messages=[{'role': 'user', 'content': 'Give context words for this query\n' + query }],
                options={
                    'num_predict': 64,      
                    'temperature': 0.5,      
                    'top_k': 20,           
                    'top_p': 0.9,
                    'repeat_penalty': 1.2,
                    'num_thread': 800, 
                    'num_gpu': 5000,  
                }
            )
            self.result = response['message']['content']
            if callback:
                callback(i, query, self.result)
        
        thread = threading.Thread(target=worker)
        thread.start()
        
        self.threads.append(thread)
    

    def wait(self):
        for t in self.threads:
            t.join()

###########################

# DenseRetrievalExactSearch is parent class for any dense model that can be used for retrieval
# Abstract class is BaseSearch
class DenseRetrievalExactSearch(BaseSearch):
    
    def __init__(self, model, batch_size: int = 128, type = "semantic", corpus_chunk_size: int = 50000, **kwargs):
        #model is class that provides encode_corpus() and encode_queries()
        self.model = model
        self.batch_size = batch_size
        self.score_functions = {'cos_sim': cos_sim, 'dot': dot_score}
        self.score_function_desc = {'cos_sim': "Cosine Similarity", 'dot': "Dot Product"}
        self.corpus_chunk_size = corpus_chunk_size
        self.show_progress_bar = kwargs.get("show_progress_bar", True)
        self.convert_to_tensor = kwargs.get("convert_to_tensor", True)
        self.results = {}
        self.type = type
    
 # MY CODE START

    def lexical_search(self, 
                       corpus: Dict[str, Dict[str, str]], 
                       queries: Dict[str, str], 
                       top_k: int,  
                       **kwargs) -> Dict[str, Dict[str, float]]:
        
        query_ids = list(queries.keys())
        self.results = {qid: {} for qid in query_ids}
        queries = [queries[qid] for qid in queries] # запросы текстом
        
        corpus_ids = sorted(corpus, key=lambda k: len(corpus[k].get("title", "") + corpus[k].get("text", "")), reverse=True)
        corpus = [corpus[cid] for cid in corpus_ids]

        docs = [doc['text'] for doc in corpus] #документы текстом

        result_heaps = {qid: [] for qid in query_ids}  # Keep only the top-k docs for each query
        for query_iter in range(len(queries)):
            query_id = query_ids[query_iter]
            q_set = set(queries[query_iter].split())
            for doc_itr in range(len(docs)):
                corpus_id = corpus_ids[doc_itr]
                d_set = set(docs[doc_itr].split())
                intersection = len(q_set.intersection(d_set))
                union = len(q_set.union(d_set))
                score = intersection / union if union > 0 else 0.0 # рассчёт score!!! (0 <= score <=1)
                if corpus_id != query_id:
                        if len(result_heaps[query_id]) < top_k:
                            # Push item on the heap
                            heapq.heappush(result_heaps[query_id], (score, corpus_id))
                        else:
                            # If item is larger than the smallest in the heap, push it on the heap then pop the smallest element
                            heapq.heappushpop(result_heaps[query_id], (score, corpus_id))

        for qid in result_heaps:
            for score, corpus_id in result_heaps[qid]:
                self.results[qid][corpus_id] = score 

        return self.results       


# MY CODE START

    def lexical_search_fh(self, 
                       corpus: Dict[str, Dict[str, str]], 
                       queries: Dict[str, str], 
                       top_k: int,  
                       **kwargs):
        
        query_ids = list(queries.keys())
        self.results = {qid: {} for qid in query_ids}
        queries = [queries[qid] for qid in queries] # запросы текстом
        
        corpus_ids = sorted(corpus, key=lambda k: len(corpus[k].get("title", "") + corpus[k].get("text", "")), reverse=True)
        corpus = [corpus[cid] for cid in corpus_ids]

        docs = [doc['text'] for doc in corpus] #документы текстом

        result_heaps = {qid: [] for qid in query_ids}  # Keep only the top-k docs for each query
        for query_iter in range(len(queries)):
            query_id = query_ids[query_iter]
            q_set = set(queries[query_iter].split())
            for doc_itr in range(len(docs)):
                corpus_id = corpus_ids[doc_itr]
                d_set = set(docs[doc_itr].split())
                intersection = len(q_set.intersection(d_set))
                union = len(q_set.union(d_set))
                score = intersection / union if union > 0 else 0.0 # рассчёт score!!! (0 <= score <=1)
                if corpus_id != query_id:
                        if len(result_heaps[query_id]) < top_k:
                            # Push item on the heap
                            heapq.heappush(result_heaps[query_id], (score, corpus_id))
                        else:
                            # If item is larger than the smallest in the heap, push it on the heap then pop the smallest element
                            heapq.heappushpop(result_heaps[query_id], (score, corpus_id))

        return result_heaps       

    def search_fh(self, 
               corpus: Dict[str, Dict[str, str]], 
               queries: Dict[str, str], 
               top_k: int, 
               score_function: str,
               return_sorted: bool = False, 
               **kwargs):
        # Create embeddings for all queries using model.encode_queries()
        # Runs semantic search against the corpus embeddings
        # Returns a ranked list with the corpus ids
        if score_function not in self.score_functions:
            raise ValueError("score function: {} must be either (cos_sim) for cosine similarity or (dot) for dot product".format(score_function))
            
       
        query_ids = list(queries.keys())
        self.results = {qid: {} for qid in query_ids}
        queries = [queries[qid] for qid in queries]
        query_embeddings = self.model.encode_queries(
            queries, batch_size=self.batch_size, show_progress_bar=self.show_progress_bar, convert_to_tensor=self.convert_to_tensor)
          

        corpus_ids = sorted(corpus, key=lambda k: len(corpus[k].get("title", "") + corpus[k].get("text", "")), reverse=True)
        corpus = [corpus[cid] for cid in corpus_ids]


        itr = range(0, len(corpus), self.corpus_chunk_size)
        
        result_heaps = {qid: [] for qid in query_ids}  # Keep only the top-k docs for each query
        for batch_num, corpus_start_idx in enumerate(itr):
            corpus_end_idx = min(corpus_start_idx + self.corpus_chunk_size, len(corpus))

            # Encode chunk of corpus    
            sub_corpus_embeddings = self.model.encode_corpus(
                corpus[corpus_start_idx:corpus_end_idx],
                batch_size=self.batch_size,
                show_progress_bar=self.show_progress_bar, 
                convert_to_tensor = self.convert_to_tensor
                )

            # Compute similarites using either cosine-similarity or dot product
            cos_scores = self.score_functions[score_function](query_embeddings, sub_corpus_embeddings)
            cos_scores[torch.isnan(cos_scores)] = -1

            # Get top-k values
            cos_scores_top_k_values, cos_scores_top_k_idx = torch.topk(cos_scores, min(top_k+1, len(cos_scores[1])), dim=1, largest=True, sorted=return_sorted)
            cos_scores_top_k_values = cos_scores_top_k_values.cpu().tolist()
            cos_scores_top_k_idx = cos_scores_top_k_idx.cpu().tolist()
            
            for query_itr in range(len(query_embeddings)):
                query_id = query_ids[query_itr]                  
                for sub_corpus_id, score in zip(cos_scores_top_k_idx[query_itr], cos_scores_top_k_values[query_itr]):
                    corpus_id = corpus_ids[corpus_start_idx+sub_corpus_id]
                    if corpus_id != query_id:
                        if len(result_heaps[query_id]) < top_k:
                            # Push item on the heap
                            heapq.heappush(result_heaps[query_id], (score, corpus_id))
                        else:
                            # If item is larger than the smallest in the heap, push it on the heap then pop the smallest element
                            heapq.heappushpop(result_heaps[query_id], (score, corpus_id))                
        
        return result_heaps

    def hibrid_search(self, 
               corpus: Dict[str, Dict[str, str]], 
               queries: Dict[str, str], 
               top_k: int, 
               score_function: str,
               return_sorted: bool = False, 
               **kwargs) -> Dict[str, Dict[str, float]]:
        

        lexical_result = self.lexical_search_fh(corpus, queries, top_k)

        semantic_result = self.search_fh(corpus, queries, top_k, score_function, return_sorted, **kwargs)

        #merge

        for qid in lexical_result:
            heapq.heapify_max(lexical_result[qid])
            heapq.heapify_max(semantic_result[qid])
            if top_k > len(lexical_result[qid]):
                border = len(lexical_result[qid])
            else:
                border = top_k
            count = 0
            addedCorpusIds = []
            f1 = 0
            while count < border/10:
                score, corpus_id = heapq.heappop_max(semantic_result[qid])
                self.results[qid][corpus_id] = score
                addedCorpusIds.append(corpus_id)
                count+=1
                f1 += 1
            f2 = 0
            while count < border:
                score, corpus_id = heapq.heappop_max(lexical_result[qid])
                if(corpus_id not in addedCorpusIds):
                    self.results[qid][corpus_id] = score
                    count+=1
                    f2 += 1

            print(f1, f2)
        #for qid in lexical_result:           
        #    for score, corpus_id in semantic_result[qid]:
        #        if corpus_id in self.results[qid]:
        #            if self.results[qid][corpus_id] < score:
        #                self.results[qid][corpus_id] = score 
        #        else:
        #            self.results[qid][corpus_id] = score
        #
        #    for score, corpus_id in lexical_result[qid]:
        #        if corpus_id in self.results[qid]:
        #            if self.results[qid][corpus_id] < score:
        #                self.results[qid][corpus_id] = score 
        #        else:
        #            self.results[qid][corpus_id] = score

        return self.results

            


  # MY CODE END


    def search(self, 
               corpus: Dict[str, Dict[str, str]], 
               queries: Dict[str, str], 
               top_k: int, 
               score_function: str,
               return_sorted: bool = False,
               useLLm: bool = False, 
               llmname="",
               **kwargs) -> Dict[str, Dict[str, float]]:
        # Create embeddings for all queries using model.encode_queries()
        # Runs semantic search against the corpus embeddings
        # Returns a ranked list with the corpus ids
        if score_function not in self.score_functions:
            raise ValueError("score function: {} must be either (cos_sim) for cosine similarity or (dot) for dot product".format(score_function))
            
        dict_queries = queries.copy()
        query_ids = list(queries.keys())
        self.results = {qid: {} for qid in query_ids}
        queries = [queries[qid] for qid in queries]
        ############################## MODDED PART FOR CCQ (CONTEXT COMPLETED QUERIES) ########################
        if(useLLm):
            new_queries = dict()
            print(len(queries))
            ollama_client = AsyncOllama(llmname)
            def callback(i, query, result):
                    new_queries[i] = query + "\n" + result
                    print(f"end{i}")
            for i, query in enumerate(queries):
                
                ollama_client.ask_async(i, query, callback)
                print(i)
            ollama_client.wait()
            queries = [new_queries[i] for i in range(len(new_queries))]
            dict_queries = dict(zip(query_ids,queries))
        ##################################################################

        
        match self.type:
            case "lexical":
                return self.lexical_search(corpus, dict_queries, top_k)

            case "semantic":
                 ################## semantic part
                query_embeddings = self.model.encode_queries(
                    queries, batch_size=self.batch_size, show_progress_bar=self.show_progress_bar, convert_to_tensor=self.convert_to_tensor)
                
            

                corpus_ids = sorted(corpus, key=lambda k: len(corpus[k].get("title", "") + corpus[k].get("text", "")), reverse=True)
                corpus = [corpus[cid] for cid in corpus_ids]

                itr = range(0, len(corpus), self.corpus_chunk_size)
                
                result_heaps = {qid: [] for qid in query_ids}  # Keep only the top-k docs for each query
                for batch_num, corpus_start_idx in enumerate(itr):
                    corpus_end_idx = min(corpus_start_idx + self.corpus_chunk_size, len(corpus))

                    # Encode chunk of corpus    
                    sub_corpus_embeddings = self.model.encode_corpus(
                        corpus[corpus_start_idx:corpus_end_idx],
                        batch_size=self.batch_size,
                        show_progress_bar=self.show_progress_bar, 
                        convert_to_tensor = self.convert_to_tensor
                        )

                    # Compute similarites using either cosine-similarity or dot product
                    cos_scores = self.score_functions[score_function](query_embeddings, sub_corpus_embeddings)
                    cos_scores[torch.isnan(cos_scores)] = -1

                    # Get top-k values
                    cos_scores_top_k_values, cos_scores_top_k_idx = torch.topk(cos_scores, min(top_k+1, len(cos_scores[1])), dim=1, largest=True, sorted=return_sorted)
                    cos_scores_top_k_values = cos_scores_top_k_values.cpu().tolist()
                    cos_scores_top_k_idx = cos_scores_top_k_idx.cpu().tolist()
                    
                    for query_itr in range(len(query_embeddings)):
                        query_id = query_ids[query_itr]                  
                        for sub_corpus_id, score in zip(cos_scores_top_k_idx[query_itr], cos_scores_top_k_values[query_itr]):
                            corpus_id = corpus_ids[corpus_start_idx+sub_corpus_id]
                            if corpus_id != query_id:
                                if len(result_heaps[query_id]) < top_k:
                                    # Push item on the heap
                                    heapq.heappush(result_heaps[query_id], (score, corpus_id))
                                else:
                                    # If item is larger than the smallest in the heap, push it on the heap then pop the smallest element
                                    heapq.heappushpop(result_heaps[query_id], (score, corpus_id))

                for qid in result_heaps:
                    for score, corpus_id in result_heaps[qid]:
                        self.results[qid][corpus_id] = score
                
                return self.results 

            case "hybrid":
                return self.hibrid_search(corpus, dict_queries, top_k, score_function,return_sorted)

       

