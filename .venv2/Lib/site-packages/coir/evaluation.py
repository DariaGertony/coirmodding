import os
import json
import logging
import torch
from coir.beir.retrieval.evaluation import EvaluateRetrieval
from coir.beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES
from coir.beir.reranking.rerank import Rerank
from coir.beir.reranking.models.cross_encoder import CrossEncoder

logger = logging.getLogger(__name__)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class COIR:
    def __init__(self, tasks, batch_size, type="semantic"):
        self.tasks = tasks
        self.batch_size = batch_size
        self.type = type
        #######################################
        

    def run(self, model, output_folder: str, useLLm: bool, llmname: str, prompt: str,to_rerank: bool, rermodel="BAAI/bge-reranker-base"):
        results = {}
        for task_name, task_data in self.tasks.items():
            output_file = os.path.join(output_folder, f"{task_name}.json")


            corpus, queries, qrels = task_data

            # Initialize custom model
            custom_model = DRES(model, batch_size=self.batch_size, type=self.type)
            retriever = EvaluateRetrieval(custom_model, score_function="cos_sim")

            ############# 
            # init rerank models
            rerank_model = CrossEncoder(rermodel, device=device)
            reranker = Rerank(rerank_model, batch_size=12)

            ################

            # Retrieve results
            task_results = retriever.retrieve(corpus, queries, useLLm, llmname, prompt)

            #####################################################

            #RERANK
            if to_rerank:
                task_results = reranker.rerank(corpus, queries, task_results, len(queries))

            #########################

            # Evaluate results
            ndcg, map, recall, precision = retriever.evaluate(qrels, task_results, retriever.k_values)
            metrics = {
                "NDCG": ndcg,
                "MAP": map,
                "Recall": recall,
                "Precision": precision
            }

            
            # Save results
            if not useLLm:
                llmname = 'clear'

            if to_rerank:
                llmname += " reranked"
            if useLLm:
                llmname += "\n"+prompt

            

            
#################add prompt step##################################################################################

            os.makedirs(output_folder, exist_ok=True)
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            except FileNotFoundError:
                data = dict()

            where = ""
            match self.type:
                case "semantic":
                    where = model.model_name
                case "lexical":
                    where = "lexical"
                case "hybrid":
                    where = f"hybrid({model.model_name})"
                
            if(data.get(llmname, None)== None):
                llmmodel_dict = dict()
                llmmodel_dict[where] = {"metrics": metrics}
                data[llmname] = llmmodel_dict
            else:
                data[llmname][where] = {"metrics": metrics}


            with open(output_file, 'w') as json_file:
                json.dump(data, json_file, indent=4)

            logger.info(f"Results for {task_name} saved to {output_folder}")
            results[task_name] = metrics

        return results